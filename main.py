# -*- coding: utf-8 -*-
"""Untitled39.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NnYRyba8HDcoZADaWPJyUxZOqbi4nTyi
"""

!pip install numpy
!pip install matplotlib
!pip install pandas
!pip install scikit-learn
!pip install seaborn

import numpy as np  # numerical computations
import pandas as pd  # data handling and analysis
import matplotlib.pyplot as plt  # data visualization
import seaborn as sns  # advanced statistical plotting
from sklearn.datasets import load_iris  # sample dataset (Iris)
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold  # data splitting, tuning, validation
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # feature scaling and encoding
from sklearn.decomposition import PCA  # dimensionality reduction
from sklearn.neighbors import KNeighborsClassifier  # KNN classifier
from sklearn.pipeline import Pipeline  # combine preprocessing and model
from sklearn.compose import ColumnTransformer  # apply transforms to columns
from sklearn.impute import SimpleImputer  # handle missing values
from sklearn.ensemble import RandomForestClassifier  # ensemble tree model
from sklearn.linear_model import LogisticRegression  # linear classification model
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay  # model evaluation tools

titanic = sns.load_dataset('titanic')
titanic.head()

titanic.count()

features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'class', 'who', 'adult_male', 'alone']
target = 'survived'

X = titanic[features]
y = titanic[target]

y.value_counts()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

numerical_features = X_train.select_dtypes(include=['number']).columns.tolist()
categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()

numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

param_grid = {
    'classifier__n_estimators': [50, 100],
    'classifier__max_depth': [None, 10, 20],
    'classifier__min_samples_split': [2, 5]
}

# Cross-validation method
cv = StratifiedKFold(n_splits=5, shuffle=True)

model = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=cv, scoring='accuracy', verbose=2)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

plt.figure()
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d')

# Set the title and labels
plt.title('Titanic Classification Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# Show the plot
plt.tight_layout()
plt.show()

model.best_estimator_['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)

feature_importances = model.best_estimator_['classifier'].feature_importances_

# Combine the numerical and one-hot encoded categorical feature names
feature_names = numerical_features + list(model.best_estimator_['preprocessor']
                                        .named_transformers_['cat']
                                        .named_steps['onehot']
                                        .get_feature_names_out(categorical_features))

importance_df = pd.DataFrame({'Feature': feature_names,
                              'Importance': feature_importances
                             }).sort_values(by='Importance', ascending=False)

# Plotting
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.gca().invert_yaxis()
plt.title('Most Important Features in predicting whether a passenger survived')
plt.xlabel('Importance Score')
plt.show()

# Print test score
test_score = model.score(X_test, y_test)
print(f"\nTest set accuracy: {test_score:.2%}")

"""## Try another model
In practice you would want to try out different models and even revisit the data analysis to improve
your model performance. Maybe you can engineer new features or impute missing values to be able to use more data.

With Scikit-learn's powerful pipeline class, this is easy to do in a few steps.
Let's update the pipeline and the parameter grid so we can train a Logistic Regression model and compare the performance of the two models.

"""

# Replace RandomForestClassifier with LogisticRegression
pipeline.set_params(classifier=LogisticRegression(random_state=42))

# update the model's estimator to use the new pipeline
model.estimator = pipeline

# Define a new grid with Logistic Regression parameters
param_grid = {
    # 'classifier__n_estimators': [50, 100],
    # 'classifier__max_depth': [None, 10, 20],
    # 'classifier__min_samples_split': [2, 5],
    'classifier__solver' : ['liblinear'],
    'classifier__penalty': ['l1', 'l2'],
    'classifier__class_weight' : [None, 'balanced']
}

model.param_grid = param_grid

# Fit the updated pipeline with Logistic Regression
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

plt.figure()
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d')

# Set the title and labels
plt.title('Titanic Classification Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# Show the plot
plt.tight_layout()
plt.show()

coefficients = model.best_estimator_.named_steps['classifier'].coef_[0]

# Combine numerical and categorical feature names
numerical_feature_names = numerical_features
categorical_feature_names = (model.best_estimator_.named_steps['preprocessor']
                                     .named_transformers_['cat']
                                     .named_steps['onehot']
                                     .get_feature_names_out(categorical_features)
                            )
feature_names = numerical_feature_names + list(categorical_feature_names)

# Create a DataFrame for the coefficients
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': coefficients
}).sort_values(by='Coefficient', ascending=False, key=abs)  # Sort by absolute values

# Plotting
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Coefficient'].abs(), color='skyblue')
plt.gca().invert_yaxis()
plt.title('Feature Coefficient magnitudes for Logistic Regression model')
plt.xlabel('Coefficient Magnitude')
plt.show()

# Print test score
test_score = model.best_estimator_.score(X_test, y_test)
print(f"\nTest set accuracy: {test_score:.2%}")

# Try XGBoost
from xgboost import XGBClassifier
from sklearn.pipeline import Pipeline

# Replace the classifier in the pipeline with XGBoost
pipeline.set_params(classifier=XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=42))

# You might need to update the param_grid as well if you plan to tune XGBoost hyperparameters
# Define a new grid with XGBoost parameters
param_grid_xgb = {
    'classifier__n_estimators': [100, 300],
    'classifier__learning_rate': [0.01, 0.05, 0.1],
    'classifier__max_depth': [3, 4, 5]
}

# Update the model's param_grid to the XGBoost grid
model.param_grid = param_grid_xgb
model.estimator = pipeline # Ensure the grid search uses the updated pipeline

# Fit the updated pipeline with XGBoost
model.fit(X_train, y_train)

# Make predictions
y_pred_xgb = model.predict(X_test)

# Evaluate the model
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))
print(classification_report(y_test, y_pred_xgb))

# Generate and display the confusion matrix
conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)
plt.figure()
sns.heatmap(conf_matrix_xgb, annot=True, cmap='Blues', fmt='d')
plt.title('XGBoost Classification Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

# Get the test accuracy for each model
rf_accuracy = model.score(X_test, y_test) # Assuming 'model' still holds the GridSearchCV object from the last fit (XGBoost)

# Need to refit the GridSearchCV for Logistic Regression and Random Forest
# Or, store the best estimators after each fit

# Let's refit and store the best estimators for comparison
print("Fitting Random Forest...")
pipeline_rf = Pipeline(steps=[('preprocessor', preprocessor),
                             ('classifier', RandomForestClassifier(random_state=42))])

param_grid_rf = {
    'classifier__n_estimators': [50, 100],
    'classifier__max_depth': [None, 10, 20],
    'classifier__min_samples_split': [2, 5]
}
grid_search_rf = GridSearchCV(estimator=pipeline_rf, param_grid=param_grid_rf, cv=cv, scoring='accuracy')
grid_search_rf.fit(X_train, y_train)
rf_accuracy = grid_search_rf.score(X_test, y_test)
print(f"Random Forest Test Accuracy: {rf_accuracy:.2%}")

print("\nFitting Logistic Regression...")
pipeline_lr = Pipeline(steps=[('preprocessor', preprocessor),
                             ('classifier', LogisticRegression(random_state=42))])
param_grid_lr = {
    'classifier__solver' : ['liblinear'],
    'classifier__penalty': ['l1', 'l2'],
    'classifier__class_weight' : [None, 'balanced']
}
grid_search_lr = GridSearchCV(estimator=pipeline_lr, param_grid=param_grid_lr, cv=cv, scoring='accuracy')
grid_search_lr.fit(X_train, y_train)
lr_accuracy = grid_search_lr.score(X_test, y_test)
print(f"Logistic Regression Test Accuracy: {lr_accuracy:.2%}")

print("\nFitting XGBoost...")
pipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor),
                              ('classifier', XGBClassifier(random_state=42))])
param_grid_xgb = {
    'classifier__n_estimators': [100, 300],
    'classifier__learning_rate': [0.01, 0.05, 0.1],
    'classifier__max_depth': [3, 4, 5]
}
grid_search_xgb = GridSearchCV(estimator=pipeline_xgb, param_grid=param_grid_xgb, cv=cv, scoring='accuracy')
grid_search_xgb.fit(X_train, y_train)
xgb_accuracy = grid_search_xgb.score(X_test, y_test)
print(f"XGBoost Test Accuracy: {xgb_accuracy:.2%}")

print("\n--- Model Comparison ---")
print(f"Random Forest Accuracy: {rf_accuracy:.2%}")
print(f"Logistic Regression Accuracy: {lr_accuracy:.2%}")
print(f"XGBoost Accuracy: {xgb_accuracy:.2%}")

from google.colab import drive
drive.mount('/content/drive')



"""for betteer accrucy

"""

# Handle missing values better
titanic['age'] = titanic['age'].fillna(titanic['age'].median())
titanic['embarked'] = titanic['embarked'].fillna(titanic['embarked'].mode()[0])
titanic['fare'] = titanic['fare'].fillna(titanic['fare'].median())

# Convert categorical columns
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
titanic['sex'] = le.fit_transform(titanic['sex'])
titanic['embarked'] = le.fit_transform(titanic['embarked'])

titanic = titanic.drop(['deck', 'embark_town', 'alive', 'class', 'who', 'adult_male'], axis=1)

# Create new features
titanic['FamilySize'] = titanic['sibsp'] + titanic['parch'] + 1
titanic['IsAlone'] = (titanic['FamilySize'] == 1).astype(int)
titanic['Age*Class'] = titanic['age'] * titanic['pclass']

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

# Define the pipeline with the preprocessor and the classifier
pipeline_rf_tuned = Pipeline(steps=[('preprocessor', preprocessor),
                                    ('classifier', RandomForestClassifier(random_state=42))])

param_grid = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [4, 6, 8, 10],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__bootstrap': [True, False]
}

grid = GridSearchCV(pipeline_rf_tuned, param_grid, cv=3, scoring='accuracy', n_jobs=-1)

# Fit the grid search to the data
grid.fit(X_train, y_train)

print("Best Random Forest Parameters:", grid.best_params_)
print("Best Accuracy:", grid.best_score_)

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

# Fit the preprocessor on the training data
preprocessor.fit(X_train)

# Apply preprocessing to the training and test data
X_train_processed = preprocessor.transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Define the individual estimators
# Use the best tuned Random Forest estimator from the grid search
rf_tuned = grid.best_estimator_['classifier']

# Define Logistic Regression and XGBoost estimators
# Ensure these are estimators that can work with the preprocessed data
lr = LogisticRegression(max_iter=1000, random_state=42) # Add random_state for reproducibility
xgb = XGBClassifier(random_state=42) # Add random_state for reproducibility

voting = VotingClassifier(
    estimators=[
        ('rf', rf_tuned),  # tuned Random Forest classifier
        ('lr', lr),
        ('xgb', xgb)
    ],
    voting='soft'
)

# Fit the voting classifier on the preprocessed data
voting.fit(X_train_processed, y_train)

# Evaluate the ensemble model on the preprocessed test data
print("Ensemble Accuracy:", voting.score(X_test_processed, y_test))

# Create a DataFrame to hold the accuracies
accuracy_data = {'Model': ['Random Forest', 'Logistic Regression', 'XGBoost', 'Ensemble'],
                 'Accuracy': [rf_accuracy, lr_accuracy, xgb_accuracy, voting.score(X_test_processed, y_test)]}
accuracy_df = pd.DataFrame(accuracy_data)

# Print the numerical table
print("--- Model Accuracy Comparison ---")
print(accuracy_df)

# Plot the accuracies
plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='Accuracy', data=accuracy_df, palette='viridis')
plt.title('Model Accuracy Comparison')
plt.ylabel('Accuracy')
plt.ylim(0.7, 0.9) # Adjust y-limit for better visualization of differences
plt.show()



# Save the titanic DataFrame to a CSV file
titanic.to_csv('titanic_dataset.csv', index=False)

# Provide a link to download the file
from google.colab import files
files.download('titanic_dataset.csv')
